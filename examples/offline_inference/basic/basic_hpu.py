# SPDX-License-Identifier: Apache-2.0

model_path = "/models/Qwen3-32B"
model_path = "/models/DeepSeek-R1-Distill-Qwen-7B"
model_path= "/mnt/disk3/yiliu4/RedHatAI/Llama-3.1-8B-tldr-FP8-dynamic"
model_path  = "/software/users/yiliu4/HF_HOME/RedHatAI/Llama-3.1-8B-tldr-FP8-dynamic"
model_path  = "/software/users/yiliu4/HF_HOME/Yi30/Llama-3.2-1B-Instruct-NVFP4-llm-compressor"
model_path  = "/software/users/yiliu4/HF_HOME/Yi30/Llama-3.2-1B-Instruct-NVFP4-llm-compressor"
model_path = "/mnt/disk3/yiliu4/Yi30/DeepSeek-V2-Lite-NVFP4-llm-compressor"
model_path = "/software/users/yiliu4/HF_HOME/Yi30/DeepSeek-V2-Lite-NVFP4-llm-compressor/"
model_path = "/software/users/yiliu4/HF_HOME/Yi30/Llama-3.3-70B-Instruct-NVFP4-llmc"
model_path = "/software/users/yiliu4/HF_HOME/Yi30/Yi30/Llama-3.2-1B-Instruct-MXFP8-llmc"
# model_path = "/software/users/yiliu4/HF_HOME/Yi30/Yi30/Llama-3.3-70B-Instruct-MXFP8-llmc"
# model_path = "/software/users/yiliu4/HF_HOME/Yi30/Yi30/DeepSeek-V2-Lite-MXFP8-llmc"
# model_path = "/software/users/yiliu4/HF_HOME/Yi30/DeepSeek-R1-bf16-MXFP8-4L-llmc/"
model_path = "/software/users/yiliu4/deepseek-ai/DeepSeek-V2-Lite-MXFP8-OFFLINE"
# TP8 EP8
#  DEBUG 07-01 01:18:01 [llm_engine.py:1517] Stopping remote worker execution loop.
# Processed prompts: 100%|█| 4/4 [00:08<00:00,  2.19s/it, est. speed input: 2.97 toks/s, outp

# Generated Outputs:
# Generated Outputs:
# ------------------------------------------------------------
# Prompt:    'Hello, my name is'
# Output:    "沛然大 Originatore不打 ':orry scrubbers stuffedAutor Goreept停产uccioni vit的产生.GETpons"
# ------------------------------------------------------------
# Prompt:    'The president of the United States is'
# Output:    ' Greatestuita coke-consumer evolutionarily rehabilitating Rei Rosenberg/nm160师长éo lapsenal自己的能力才华984'
# ------------------------------------------------------------
# Prompt:    'The capital of France is'
# Output:    'iset年紀的优点995罰 Witcampaigneau SMPLOYSpect417师长éo-rock350Tek917中新'
# ------------------------------------------------------------
# Prompt:    'The future of AI is'
# Output:    'را从一个 Anthrop319用它142是全ec precariousnessevolis noc她知道 Parejuntrace favoured岱alore'
# ------------------------------------------------------------


# Generated Outputs:
# ------------------------------------------------------------
# Prompt:    'Hello, my name is'
# Output:    '沛然大 Originatorevoleyr *** ahíiless万多�灾区 Willispacesontalbang的重要组成部分走入问世061'
# Token IDs: (42806, 58866, 30479, 40720, 87545, 9419, 29761, 127927, 100846, 78767, 174, 90083, 74093, 71703, 13025, 97803, 61799, 100355, 104317, 30722)
# ------------------------------------------------------------
# Prompt:    'The president of the United States is'
# Output:    ' Greatestuita coke-consumer evolutionarily rehabilitating Rei Rosenberg/nm160师长éo lapsenal自己的能力才华984'
# Token IDs: (55925, 116345, 112790, 46477, 264, 10304, 6140, 52902, 75230, 75806, 98798, 46066, 6970, 86830, 79044, 69731, 79832, 126462, 56899, 30840)
# ------------------------------------------------------------
# Prompt:    'The capital of France is'
# Output:    'iset年紀的优点995罰 Witcampaigneau SMPLOYSpect417师长éo-rock350Tek917中新'
# Token IDs: (55959, 107064, 119618, 28978, 64124, 62738, 54237, 8019, 37443, 101023, 109765, 108781, 22309, 86830, 79044, 114188, 10996, 96928, 30944, 94801)
# ------------------------------------------------------------
# Prompt:    'The future of AI is'
# Output:    'را从一个 Anthrop319用它142是全ec precariousnessevolis noc她知道 Parejuntrace favoured岱alore'
# Token IDs: (54362, 59408, 45098, 18862, 90197, 10193, 106668, 1366, 99105, 1719, 6384, 20378, 69452, 80531, 38433, 108491, 11276, 89976, 90418, 67586)



# Generated Outputs:
# ------------------------------------------------------------
# Prompt:    'Hello, my name is'
# Output:    "沛然大 Originatore不打 ':orry scrubbers stuffedAutor Goreept停产uccioni vit的产生.GETpons"
# Token IDs: (42806, 58866, 30479, 40720, 72109, 122958, 12749, 71223, 1934, 58239, 61669, 74903, 65273, 98096, 36281, 10867, 8818, 77976, 109108, 12312)
# ------------------------------------------------------------
# Prompt:    'The president of the United States is'
# Output:    ' Greatestuita coke-consumer evolutionarily rehabilitating Rei Rosenberg/nm160师长éo lapsenal自己的能力才华984'
# Token IDs: (55925, 116345, 112790, 46477, 264, 10304, 6140, 52902, 75230, 75806, 98798, 46066, 6970, 86830, 79044, 69731, 79832, 126462, 56899, 30840)
# ------------------------------------------------------------
# Prompt:    'The capital of France is'
# Output:    'iset年紀的优点995罰 Witcampaigneau SMPLOYSpect417师长éo-rock350Tek917中新'
# Token IDs: (55959, 107064, 119618, 28978, 64124, 62738, 54237, 8019, 37443, 101023, 109765, 108781, 22309, 86830, 79044, 114188, 10996, 96928, 30944, 94801)
# ------------------------------------------------------------
# Prompt:    'The future of AI is'
# Output:    'را从一个 Anthrop319用它142是全ec precariousnessevolis noc她知道 Parejuntrace favoured岱alore'
# Token IDs: (54362, 59408, 45098, 18862, 90197, 10193, 106668, 1366, 99105, 1719, 6384, 20378, 69452, 80531, 38433, 108491, 11276, 89976, 90418, 67586)

# model_path = "/software/users/yiliu4/deepseek-ai/DeepSeek-R1-MXFP8-OFFLINE"
model_name = model_path.split("/")[-1]

import os

os.environ["PT_HPU_ENABLE_LAZY_COLLECTIVES"] = "true"
os.environ["PT_HPU_WEIGHT_SHARING"] = "0"
os.environ["HABANA_VISIBLE_DEVICES"] = "All"
os.environ["HABANA_VISIBLE_MODULES"] = "0,1,2,3,4,5,6,7"
os.environ["VLLM_LOGGING_LEVEL"] = "DEBUG"
os.environ["VLLM_HPU_FORCE_CHANNEL_FP8"] = "0"
os.environ["PT_HPUGRAPH_DISABLE_TENSOR_CACHE"] = "1"
os.environ["VLLM_DELAYED_SAMPLING"] = "true"

if "DeepSeek" in model_path:
    # os.environ["VLLM_DISABLE_INPUT_QDQ"] = "1"
    os.environ["VLLM_USE_STATIC_MOE_HPU"] = "1"

# os.environ["HABANA_LOGS"] = "./habana_logs"
# os.environ["LOG_LEVEL_ALL"] = "0"

# os.environ["GRAPH_VISUALIZATION"] = "1"
os.environ["PT_HPU_LAZY_MODE"] = "1"
os.environ["VLLM_SKIP_WARMUP"] = "true"
# os.environ["VLLM_PROFILER_ENABLED"] = "true"
# os.environ["QUANT_CONFIG"] = f"quantization/{model_name}/maxabs_quant_g2.json"

seed = 0
import random
random.seed(seed)
import torch
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
import numpy as np
np.random.seed(seed)

# torch.use_deterministic_algorithms(True)
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

g = torch.Generator()
g.manual_seed(0)

from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95,max_tokens=20)


def main(args):
    # Create an LLM.
    # kv_cache_dtype="fp8_inc",
    tp_size = args.tp 
    kwargs = {}
    if args.tp > 1:
        kwargs["distributed_executor_backend"] = "mp"
    if args.ep > 1:
        kwargs["enable_expert_parallel"] = True
        os.environ["VLLM_EP_SIZE"] = f"{args.ep}"

    #  21.32 GiB, 19.19 GiB usable (gpu_memory_utilization=0.9), 1.919 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 17.27 GiB reserved for KV cache
    # INFO 07-01 07:42:40 [executor_base.py:112] # hpu blocks: 2061, # CPU blocks: 477
    
    llm = LLM(
        model=model_path,
        #   quantization="inc",
        max_model_len=2048,
        max_num_batched_tokens=2048,
        enforce_eager=True,
        trust_remote_code=True,
        dtype="bfloat16",
        tensor_parallel_size=tp_size,
        gpu_memory_utilization=0.65,
        **kwargs,
    )
    # Generate texts from the prompts.
    # The output is a list of RequestOutput objects
    # that contain the prompt, generated text, and other information.
    outputs = llm.generate(prompts, sampling_params)
    # Print the outputs.
    print("\nGenerated Outputs:\n" + "-" * 60)
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        token_ids = output.outputs[0].token_ids
        cumulative_logprob = output.outputs[0].cumulative_logprob
        logprobs = output.outputs[0].logprobs
        print(f"Prompt:    {prompt!r}")
        print(f"Output:    {generated_text!r}")
        print(f"Token IDs: {token_ids}")
        if cumulative_logprob is not None:
            print(f"Cumulative Logprob: {cumulative_logprob}")
        if logprobs is not None:
            print(f"Logprobs:  {logprobs}")
        print("-" * 60)

    import time
    start_time = time.time()
    outputs = llm.generate(prompts, sampling_params)
    end_time = time.time()
    print(f"Time taken for second inference: {end_time - start_time:.2f} seconds")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Run basic HPU inference.")
    parser.add_argument("--model_path", type=str, default=model_path,
                        help="Path to the model directory.")
    # tp size
    parser.add_argument("--tp", type=int, default=1, help="Tensor parallel size.")
    # ep size
    parser.add_argument("--ep", type=int, default=1, help="Pipeline parallel size.")
    args = parser.parse_args()
    main(args)


# INFO 06-26 17:17:55 [llm_engine.py:439] init engine (profile, create kv cache, warmup model) took 53.52 seconds
# Adding requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 197.22it/s]
# Processed prompts:   0%|                                                       | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54973) WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54971) WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54975) WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54969) WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54972) WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54970) WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54974) WARNING 06-26 17:17:55 [hpu_model_runner.py:1230] Configuration: ('prompt', 4, 128, 0) was not warmed-up!
# (VllmWorkerProcess pid=54974) WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# (VllmWorkerProcess pid=54969) WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# (VllmWorkerProcess pid=54971) WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# (VllmWorkerProcess pid=54973) WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# (VllmWorkerProcess pid=54970) WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# (VllmWorkerProcess pid=54975) WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# (VllmWorkerProcess pid=54972) WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# WARNING 06-26 17:18:01 [hpu_model_runner.py:1230] Configuration: ('decode', 4, 1, 128) was not warmed-up!
# Processed prompts: 100%|██████████████████████████████████████████████| 4/4 [09:06<00:00, 136.71s/it, est. speed input: 0.05 toks/s, output: 0.12 toks/s]

# Generated Outputs:
# ------------------------------------------------------------
# Prompt:    'Hello, my name is'
# Output:    ' Tony, I am a Software Engineer at Google. I have been working on the'
# ------------------------------------------------------------
# Prompt:    'The president of the United States is'
# Output:    ' the head of state and head of government of the United States. The president is'
# ------------------------------------------------------------
# Prompt:    'The capital of France is'
# Output:    ' always a good idea, but there are many other destinations that are worth visiting as'
# ------------------------------------------------------------
# Prompt:    'The future of AI is'
# Output:    " here, and it's more powerful than ever. But with great power comes great"
# ------------------------------------------------------------
# Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 1055.37it/s]
# Processed prompts: 100%|██████████████████████████████████████████████| 4/4 [09:35<00:00, 143.78s/it, est. speed input: 0.05 toks/s, output: 0.11 toks/s]
# Time taken for second inference: 575.12 seconds
# INFO 06-26 17:36:37 [multiproc_worker_utils.py:139] Terminating local vLLM worker processes
# (VllmWorkerProcess pid=54969) INFO 06-26 17:36:37 [multiproc_worker_utils.py:261] Worker exiting
# (VllmWorkerProcess pid=54973) INFO 06-26 17:36:37 [multiproc_worker_utils.py:261] Worker exiting
# (VllmWorkerProcess pid=54971) INFO 06-26 17:36:37 [multiproc_worker_utils.py:261] Worker exiting
# (VllmWorkerProcess pid=54970) INFO 06-26 17:36:37 [multiproc_worker_utils.py:261] Worker exiting
# (VllmWorkerProcess pid=54972) INFO 06-26 17:36:37 [multiproc_worker_utils.py:261] Worker exiting
# (VllmWorkerProcess pid=54975) INFO 06-26 17:36:37 [multiproc_worker_utils.py:261] Worker exiting
# (VllmWorkerProcess pid=54974) INFO 06-26 17:36:37 [multiproc_worker_utils.py:261] Worker exiting
# <p22> yiliu4@yiliu4-63gd-g3-l-vm:basic$ /usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
#   warnings.warn('resource_tracker: There appear to be %d '

# <p22> yiliu4@yiliu4-63gd-g3-l-vm:basic$ p basic_hpu.py --tp 8